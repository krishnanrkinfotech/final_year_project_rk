# -*- coding: utf-8 -*-
"""preprocessing _kdd

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TJSQ9_-XH7VOkFj1jfRUyPSUakLccq2b
"""

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt

df = pd.read_csv('/content/drive/My Drive/kddcup99_csv.csv')

df.head()

df.describe()

import seaborn as sns
import matplotlib.pyplot as plt
#get correlations of each features in dataset
corrmat = df.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap="RdYlGn")

"""Removal of redundant features¶"""

df['lnum_outbound_cmds'].value_counts()
df.drop('lnum_outbound_cmds', axis=1, inplace=True)
df['is_host_login'].value_counts()
df.drop('is_host_login', axis=1, inplace=True)
df['wrong_fragment'].value_counts()
df.drop('wrong_fragment', axis=1, inplace=True)
df['hot'].value_counts()
df.drop('hot', axis=1, inplace=True)
df['num_failed_logins'].value_counts()
df.drop('num_failed_logins', axis=1, inplace=True)
df['logged_in'].value_counts()
df.drop('logged_in', axis=1, inplace=True)
df['lroot_shell'].value_counts()
df.drop('lroot_shell', axis=1, inplace=True)
df['lnum_file_creations'].value_counts()
df.drop('lnum_file_creations', axis=1, inplace=True)
df['lnum_shells'].value_counts()
df.drop('lnum_shells', axis=1, inplace=True)
df['lnum_access_files'].value_counts()
df.drop('lnum_access_files', axis=1, inplace=True)
df['is_guest_login'].value_counts()
df.drop('is_guest_login', axis=1, inplace=True)
df['serror_rate'].value_counts()
df.drop('serror_rate', axis=1, inplace=True)
df['srv_serror_rate'].value_counts()
df.drop('srv_serror_rate', axis=1, inplace=True)
df['rerror_rate'].value_counts()
df.drop('rerror_rate', axis=1, inplace=True)
df['srv_rerror_rate'].value_counts()
df.drop('srv_rerror_rate', axis=1, inplace=True)
df['same_srv_rate'].value_counts()
df.drop('same_srv_rate', axis=1, inplace=True)
df['diff_srv_rate'].value_counts()
df.drop('diff_srv_rate', axis=1, inplace=True)
df['srv_diff_host_rate'].value_counts()
df.drop('srv_diff_host_rate', axis=1, inplace=True)
df['dst_host_same_srv_rate'].value_counts()
df.drop('dst_host_same_srv_rate', axis=1, inplace=True)
df['dst_host_diff_srv_rate'].value_counts()
df.drop('dst_host_diff_srv_rate', axis=1, inplace=True)
df['dst_host_same_src_port_rate'].value_counts()
df.drop('dst_host_same_src_port_rate', axis=1, inplace=True)
df['dst_host_srv_diff_host_rate'].value_counts()
df.drop('dst_host_srv_diff_host_rate', axis=1, inplace=True)
df['dst_host_serror_rate'].value_counts()
df.drop('dst_host_serror_rate', axis=1, inplace=True)
df['dst_host_srv_serror_rate'].value_counts()
df.drop('dst_host_srv_serror_rate', axis=1, inplace=True)
df['dst_host_rerror_rate'].value_counts()
df.drop('dst_host_rerror_rate', axis=1, inplace=True)
df['dst_host_srv_rerror_rate'].value_counts()
df.drop('dst_host_srv_rerror_rate', axis=1, inplace=True)
df['lsu_attempted'].value_counts()
df.drop('lsu_attempted', axis=1, inplace=True)
df['urgent'].value_counts()
df.drop('urgent', axis=1, inplace=True)

#df['lnum_outbound_cmds'].value_counts()
#df.drop('lnum_outbound_cmds', axis=1, inplace=True)
#df['is_host_login'].value_counts()
#df.drop('is_host_login', axis=1, inplace=True)

df['protocol_type'] = df['protocol_type'].astype('category')
df['service'] = df['service'].astype('category')
df['flag'] = df['flag'].astype('category')
cat_columns = df.select_dtypes(['category']).columns
df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)

"""Removal of duplicates"""

df.drop_duplicates(subset=None, keep='first', inplace=True)

df.shape

df['label'].value_counts()

"""Log-scaled distribution of attacks"""

plt.clf()
plt.figure(figsize=(12,8))
params = {'axes.titlesize':'18',
          'xtick.labelsize':'14',
          'ytick.labelsize':'14'}
matplotlib.rcParams.update(params)
plt.title('Distribution of attacks')
#df.plot(kind='barh')
df['label'].value_counts().apply(np.log).plot(kind='barh')

plt.show()

"""KDD skewness and kurtosis"""

df.skew()

df.kurtosis()

"""Univariate histogramms"""

import matplotlib.pyplot as plt
import matplotlib
params = {'axes.titlesize':'28',
          'xtick.labelsize':'24',
          'ytick.labelsize':'24'}
matplotlib.rcParams.update(params)
df.hist(figsize=(50, 30), bins=20)
plt.show()

"""KDD standardization"""

df.shape

data = df.values

X = data[:, 0:13]

X

from sklearn.preprocessing import StandardScaler
sScaler = StandardScaler()
rescaleX = sScaler.fit_transform(X)

rescaleX

df_rescaled = pd.DataFrame(data=rescaleX)

df_rescaled.hist(figsize=(50, 30), bins=20)
plt.show()

"""KDD normalization¶"""

from sklearn.preprocessing import Normalizer
norm = Normalizer()
xNormalize = norm.fit_transform(X)

xNormalize

df_Normalized = pd.DataFrame(data=xNormalize)

df_Normalized.hist(figsize=(50, 30), bins=20)
plt.show()

"""Encoding"""

from sklearn.preprocessing import OneHotEncoder, LabelEncoder

df['label'] = df['label'].astype('category')
cat_columns = df.select_dtypes(['category']).columns
df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)

data = df.values

Y = data[:,13]

X = data[:,0:14]

Y

X

X = np.transpose(X)
X

df.shape

from sklearn.decomposition import PCA
pca = PCA(n_components=3)

pca.fit(X,Y)

pca.components_

pca.explained_variance_

pca.transform(X)

